<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Sundaresh Iyer - Enhancing Targeting Accuracy Using ML</title>
    <link rel="stylesheet" href="/sundareshiyer.github.io/assets/css/main.css?v=1">
</head>
<body>
    <header>
        <div class="header-content">
            <div class="title-description">
                <h1>Sundaresh Iyer</h1>
                <h2>Portfolio of Work</h2>
            </div>
        </div>
    </header>
    <nav>
        <a href="/sundareshiyer.github.io/">Portfolio</a>
        <a href="/sundareshiyer.github.io/assets/resume.pdf">Resume</a>
        <a href="https://linkedin.com/in/sundaresh-iyer">LinkedIn</a>
        <a href="https://github.com/IyerS">GitHub</a>
    </nav>

    <main>
        <article class="post">
    <header class="post-header">
        <h1 class="post-title">Enhancing Targeting Accuracy Using ML</h1>
        <p class="post-meta">April 02, 2022</p>
        <div class="post-tags">
            
            <span class="tag">Customer Targeting</span>
            
            <span class="tag">Machine Learning</span>
            
            <span class="tag">Classification</span>
            
            <span class="tag">Python</span>
            
        </div>
    </header>

    <div class="post-content">
        <p>Our client, a grocery retailer, wants to utilise Machine Learning to reduce mailing costs, and improve ROI!</p>

<h1 id="table-of-contents">Table of contents</h1>

<ul>
  <li><a href="#overview-main">00. Project Overview</a>
    <ul>
      <li><a href="#overview-context">Context</a></li>
      <li><a href="#overview-actions">Actions</a></li>
      <li><a href="#overview-results">Results</a></li>
      <li><a href="#overview-growth">Growth/Next Steps</a></li>
    </ul>
  </li>
  <li><a href="#data-overview">01. Data Overview</a></li>
  <li><a href="#modelling-overview">02. Modelling Overview</a></li>
  <li><a href="#logreg-title">03. Logistic Regression</a></li>
  <li><a href="#clftree-title">04. Decision Tree</a></li>
  <li><a href="#rf-title">05. Random Forest</a></li>
  <li><a href="#knn-title">06. KNN</a></li>
  <li><a href="#modelling-summary">07. Modelling Summary</a></li>
  <li><a href="#modelling-application">08. Application</a></li>
  <li><a href="#growth-next-steps">09. Growth &amp; Next Steps</a></li>
</ul>

<hr />

<h1 id="project-overview--">Project Overview  <a name="overview-main"></a></h1>

<h3 id="context-">Context <a name="overview-context"></a></h3>

<p>Our client, a grocery retailer, sent out mailers in a marketing campaign for their new <em>delivery club</em>.  This cost customers $100 per year for membership, and offered free grocery deliveries, rather than the normal cost of $10 per delivery.</p>

<p>For this, they sent mailers to their entire customer base (apart from a control group) but this proved expensive.  For the next batch of communications they would like to save costs by <em>only</em> mailing customers that were likely to sign up.</p>

<p>Based upon the results of the last campaign and the customer data available, we will look to understand the <em>probability</em> of customers signing up for the <em>delivery club</em>.  This would allow the client to mail a more targeted selection of customers, lowering costs, and improving ROI.</p>

<p>Let’s use Machine Learning to take on this task!
<br />
<br /></p>
<h3 id="actions-">Actions <a name="overview-actions"></a></h3>

<p>We firstly needed to compile the necessary data from tables in the database, gathering key customer metrics that may help predict <em>delivery club</em> membership.</p>

<p>Within our historical dataset from the last campaign, we found that 69% of customers did not sign up and 31% did.  This tells us that while the data isn’t perfectly balanced at 50:50, it isn’t <em>too</em> imbalanced either.  Even so, we make sure to not rely on classification accuracy alone when assessing results - also analysing Precision, Recall, and F1-Score.</p>

<p>As we are predicting a binary output, we tested four classification modelling approaches, namely:</p>

<ul>
  <li>Logistic Regression</li>
  <li>Decision Tree</li>
  <li>Random Forest</li>
  <li>K Nearest Neighbours (KNN)</li>
</ul>

<p>For each model, we will import the data in the same way but will need to pre-process the data based up the requirements of each particular algorithm.  We will train &amp; test each model, look to refine each to provide optimal performance, and then measure this predictive performance based on several metrics to give a well-rounded overview of which is best.
<br />
<br /></p>

<h3 id="results-">Results <a name="overview-results"></a></h3>

<p>The goal for the project was to build a model that would accurately predict the customers that would sign up for the <em>delivery club</em>.  This would allow for a much more targeted approach when running the next iteration of the campaign.  A secondary goal was to understand what the drivers for this are, so the client can get closer to the customers that need or want this service, and enhance their messaging.</p>

<p>Based upon these, the chosen the model is the Random Forest as it was a) the most consistently performant on the test set across classication accuracy, precision, recall, and f1-score, and b) the feature importance and permutation importance allows the client an understanding of the key drivers behind <em>delivery club</em> signups.</p>

<p><br />
<strong>Metric 1: Classification Accuracy</strong></p>

<ul>
  <li>KNN = 0.936</li>
  <li>Random Forest = 0.935</li>
  <li>Decision Tree = 0.929</li>
  <li>Logistic Regression = 0.866</li>
</ul>

<p><br />
<strong>Metric 2: Precision</strong></p>

<ul>
  <li>KNN = 1.00</li>
  <li>Random Forest = 0.887</li>
  <li>Decision Tree = 0.885</li>
  <li>Logistic Regression = 0.784</li>
</ul>

<p><br />
<strong>Metric 3: Recall</strong></p>

<ul>
  <li>Random Forest = 0.904</li>
  <li>Decision Tree = 0.885</li>
  <li>KNN = 0.762</li>
  <li>Logistic Regression = 0.69</li>
</ul>

<p><br />
<strong>Metric 4: F1 Score</strong></p>

<ul>
  <li>Random Forest = 0.895</li>
  <li>Decision Tree = 0.885</li>
  <li>KNN = 0.865</li>
  <li>Logistic Regression = 0.734
<br />
<br />
    <h3 id="growthnext-steps-">Growth/Next Steps <a name="overview-growth"></a></h3>
  </li>
</ul>

<p>While predictive accuracy was relatively high - other modelling approaches could be tested, especially those somewhat similar to Random Forest, for example XGBoost, LightGBM to see if even more accuracy could be gained.</p>

<p>From a data point of view, further variables could be collected, and further feature engineering could be undertaken to ensure that we have as much useful information available for predicting customer loyalty
<br />
<br />
___</p>

<h1 id="data-overview--">Data Overview  <a name="data-overview"></a></h1>

<p>We will be predicting the binary <em>signup_flag</em> metric from the <em>campaign_data</em> table in the client database.</p>

<p>The key variables hypothesised to predict this will come from the client database, namely the <em>transactions</em> table, the <em>customer_details</em> table, and the <em>product_areas</em> table.</p>

<p>We aggregated up customer data from the 3 months prior to the last campaign.</p>

<p>After this data pre-processing in Python, we have a dataset for modelling that contains the following fields…
<br />
<br /></p>

<table>
  <thead>
    <tr>
      <th><strong>Variable Name</strong></th>
      <th><strong>Variable Type</strong></th>
      <th><strong>Description</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>signup_flag</td>
      <td>Dependent</td>
      <td>A binary variable showing if the customer signed up for the delivery club in the last campaign</td>
    </tr>
    <tr>
      <td>distance_from_store</td>
      <td>Independent</td>
      <td>The distance in miles from the customers home address, and the store</td>
    </tr>
    <tr>
      <td>gender</td>
      <td>Independent</td>
      <td>The gender provided by the customer</td>
    </tr>
    <tr>
      <td>credit_score</td>
      <td>Independent</td>
      <td>The customers most recent credit score</td>
    </tr>
    <tr>
      <td>total_sales</td>
      <td>Independent</td>
      <td>Total spend by the customer in ABC Grocery - 3 months pre campaign</td>
    </tr>
    <tr>
      <td>total_items</td>
      <td>Independent</td>
      <td>Total products purchased by the customer in ABC Grocery - 3 months pre campaign</td>
    </tr>
    <tr>
      <td>transaction_count</td>
      <td>Independent</td>
      <td>Total unique transactions made by the customer in ABC Grocery - 3 months pre campaign</td>
    </tr>
    <tr>
      <td>product_area_count</td>
      <td>Independent</td>
      <td>The number of product areas within ABC Grocery the customers has shopped into - 3 months pre campaign</td>
    </tr>
    <tr>
      <td>average_basket_value</td>
      <td>Independent</td>
      <td>The average spend per transaction for the customer in ABC Grocery - 3 months pre campaign</td>
    </tr>
  </tbody>
</table>

<p><br /></p>
<h1 id="modelling-overview--">Modelling Overview  <a name="modelling-overview"></a></h1>

<p>We will build a model that looks to accurately predict <em>signup_flag</em>, based upon the customer metrics listed above.</p>

<p>If that can be achieved, we can use this model to predict signup &amp; signup probability for future campaigns.  This information can be used to target those more likely to sign-up, reducing marketing costs and thus increasing ROI.</p>

<p>As we are predicting a binary output, we tested three classification modelling approaches, namely:</p>

<ul>
  <li>Logistic Regression</li>
  <li>Decision Tree</li>
  <li>Random Forest</li>
</ul>

<p><br /></p>
<h1 id="logistic-regression-">Logistic Regression <a name="logreg-title"></a></h1>

<p>We utlise the scikit-learn library within Python to model our data using Logistic Regression. The code sections below are broken up into 5 key sections:</p>

<ul>
  <li>Data Import</li>
  <li>Data Preprocessing</li>
  <li>Model Training</li>
  <li>Performance Assessment</li>
  <li>Optimal Threshold Analysis</li>
</ul>

<p><br /></p>
<h3 id="data-import-">Data Import <a name="logreg-import"></a></h3>

<p>Since we saved our modelling data as a pickle file, we import it.  We ensure we remove the id column, and we also ensure our data is shuffled.</p>

<p>We also investigate the class balance of our dependent variable - which is important when assessing classification accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="n">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFECV</span>

<span class="c1"># import modelling data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/delivery_club_modelling.p</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># drop uneccessary columns
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_id</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># shuffle data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">data_for_model</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="c1"># assess class balance of dependent variable
</span><span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">(</span><span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
From the last step in the above code, we see that <strong>69% of customers did not sign up and 31% did</strong>.  This tells us that while the data isn’t perfectly balanced at 50:50, it isn’t <em>too</em> imbalanced either.  Because of this, and as you will see, we make sure to not rely on classification accuracy alone when assessing results - also analysing Precision, Recall, and F1-Score.</p>

<p><br /></p>
<h3 id="data-preprocessing-">Data Preprocessing <a name="logreg-preprocessing"></a></h3>

<p>For Logistic Regression we have certain data preprocessing steps that need to be addressed, including:</p>

<ul>
  <li>Missing values in the data</li>
  <li>The effect of outliers</li>
  <li>Encoding categorical variables to numeric form</li>
  <li>Multicollinearity &amp; Feature Selection</li>
</ul>

<p><br /></p>
<h5 id="missing-values">Missing Values</h5>

<p>The number of missing values in the data was extremely low, so instead of applying any imputation (i.e. mean, most common value) we will just remove those rows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># remove rows where values are missing
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">data_for_model</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span> <span class="o">=</span> <span class="sh">"</span><span class="s">any</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="outliers">Outliers</h5>

<p>The ability for a Logistic Regression model to generalise well across <em>all</em> data can be hampered if there are outliers present.  There is no right or wrong way to deal with outliers, but it is always something worth very careful consideration - just because a value is high or low, does not necessarily mean it should not be there!</p>

<p>In this code section, we use <strong>.describe()</strong> from Pandas to investigate the spread of values for each of our predictors.  The results of this can be seen in the table below.</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th><strong>metric</strong></th>
      <th><strong>distance_from_store</strong></th>
      <th><strong>credit_score</strong></th>
      <th><strong>total_sales</strong></th>
      <th><strong>total_items</strong></th>
      <th><strong>transaction_count</strong></th>
      <th><strong>product_area_count</strong></th>
      <th><strong>average_basket_value</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mean</td>
      <td>2.61</td>
      <td>0.60</td>
      <td>968.17</td>
      <td>143.88</td>
      <td>22.21</td>
      <td>4.18</td>
      <td>38.03</td>
    </tr>
    <tr>
      <td>std</td>
      <td>14.40</td>
      <td>0.10</td>
      <td>1073.65</td>
      <td>125.34</td>
      <td>11.72</td>
      <td>0.92</td>
      <td>24.24</td>
    </tr>
    <tr>
      <td>min</td>
      <td>0.00</td>
      <td>0.26</td>
      <td>2.09</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>2.09</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>0.73</td>
      <td>0.53</td>
      <td>383.94</td>
      <td>77.00</td>
      <td>16.00</td>
      <td>4.00</td>
      <td>21.73</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>1.64</td>
      <td>0.59</td>
      <td>691.64</td>
      <td>123.00</td>
      <td>23.00</td>
      <td>4.00</td>
      <td>31.07</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>2.92</td>
      <td>0.67</td>
      <td>1121.53</td>
      <td>170.50</td>
      <td>28.00</td>
      <td>5.00</td>
      <td>46.43</td>
    </tr>
    <tr>
      <td>max</td>
      <td>400.97</td>
      <td>0.88</td>
      <td>7372.06</td>
      <td>910.00</td>
      <td>75.00</td>
      <td>5.00</td>
      <td>141.05</td>
    </tr>
  </tbody>
</table>

<p><br />
Based on this investigation, we see some <em>max</em> column values for several variables to be much higher than the <em>median</em> value.</p>

<p>This is for columns <em>distance_from_store</em>, <em>total_sales</em>, and <em>total_items</em></p>

<p>For example, the median <em>distance_to_store</em> is 1.64 miles, but the maximum is over 400 miles!</p>

<p>Because of this, we apply some outlier removal in order to facilitate generalisation across the full dataset.</p>

<p>We do this using the “boxplot approach” where we remove any rows where the values within those columns are outside of the interquartile range multiplied by 2.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">outlier_investigation</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">describe</span><span class="p">()</span>
<span class="n">outlier_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">distance_from_store</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">total_sales</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">total_items</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># boxplot approach
</span><span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">outlier_columns</span><span class="p">:</span>
    
    <span class="n">lower_quartile</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="n">upper_quartile</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
    <span class="n">iqr</span> <span class="o">=</span> <span class="n">upper_quartile</span> <span class="o">-</span> <span class="n">lower_quartile</span>
    <span class="n">iqr_extended</span> <span class="o">=</span> <span class="n">iqr</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">min_border</span> <span class="o">=</span> <span class="n">lower_quartile</span> <span class="o">-</span> <span class="n">iqr_extended</span>
    <span class="n">max_border</span> <span class="o">=</span> <span class="n">upper_quartile</span> <span class="o">+</span> <span class="n">iqr_extended</span>
    
    <span class="n">outliers</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[(</span><span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">min_border</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_border</span><span class="p">)].</span><span class="n">index</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">outliers</span><span class="p">)</span><span class="si">}</span><span class="s"> outliers detected in column </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">outliers</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="split-out-data-for-modelling">Split Out Data For Modelling</h5>

<p>In the next code block we do two things, we firstly split our data into an <strong>X</strong> object which contains only the predictor variables, and a <strong>y</strong> object that contains only our dependent variable.</p>

<p>Once we have done this, we split our data into training and test sets to ensure we can fairly validate the accuracy of the predictions on data that was not used in training.  In this case, we have allocated 80% of the data for training, and the remaining 20% for validation.  We make sure to add in the <em>stratify</em> parameter to ensure that both our training and test sets have the same proportion of customers who did, and did not, sign up for the <em>delivery club</em> - meaning we can be more confident in our assessment of predictive performance.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># split data into X and y objects for modelling
</span><span class="n">X</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># split out training &amp; test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="categorical-predictor-variables">Categorical Predictor Variables</h5>

<p>In our dataset, we have one categorical variable <em>gender</em> which has values of “M” for Male, “F” for Female, and “U” for Unknown.</p>

<p>The Logistic Regression algorithm can’t deal with data in this format as it can’t assign any numerical meaning to it when looking to assess the relationship between the variable and the dependent variable.</p>

<p>As <em>gender</em> doesn’t have any explicit <em>order</em> to it, in other words, Male isn’t higher or lower than Female and vice versa - one appropriate approach is to apply One Hot Encoding to the categorical column.</p>

<p>One Hot Encoding can be thought of as a way to represent categorical variables as binary vectors, in other words, a set of <em>new</em> columns for each categorical value with either a 1 or a 0 saying whether that value is true or not for that observation.  These new columns would go into our model as input variables, and the original column is discarded.</p>

<p>We also drop one of the new columns using the parameter <em>drop = “first”</em>.  We do this to avoid the <em>dummy variable trap</em> where our newly created encoded columns perfectly predict each other - and we run the risk of breaking the assumption that there is no multicollinearity, a requirement or at least an important consideration for some models, Linear Regression being one of them! Multicollinearity occurs when two or more input variables are <em>highly</em> correlated with each other, it is a scenario we attempt to avoid as in short, while it won’t neccessarily affect the predictive accuracy of our model, it can make it difficult to trust the statistics around how well the model is performing, and how much each input variable is truly having.</p>

<p>In the code, we also make sure to apply <em>fit_transform</em> to the training set, but only <em>transform</em> to the test set.  This means the One Hot Encoding logic will <em>learn and apply</em> the “rules” from the training data, but only <em>apply</em> them to the test data.  This is important in order to avoid <em>data leakage</em> where the test set <em>learns</em> information about the training data, and means we can’t fully trust model performance metrics!</p>

<p>For ease, after we have applied One Hot Encoding, we turn our training and test objects back into Pandas Dataframes, with the column names applied.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># list of categorical variables that need encoding
</span><span class="n">categorical_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># instantiate OHE class
</span><span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop</span> <span class="o">=</span> <span class="sh">"</span><span class="s">first</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># apply OHE
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>
<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>

<span class="c1"># extract feature names for encoded columns
</span><span class="n">encoder_feature_names</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">)</span>

<span class="c1"># turn objects back to pandas dataframe
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_train_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_test_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="feature-selection">Feature Selection</h5>

<p>Feature Selection is the process used to select the input variables that are most important to your Machine Learning task.  It can be a very important addition or at least, consideration, in certain scenarios.  The potential benefits of Feature Selection are:</p>

<ul>
  <li><strong>Improved Model Accuracy</strong> - eliminating noise can help true relationships stand out</li>
  <li><strong>Lower Computational Cost</strong> - our model becomes faster to train, and faster to make predictions</li>
  <li><strong>Explainability</strong> - understanding &amp; explaining outputs for stakeholder &amp; customers becomes much easier</li>
</ul>

<p>There are many, many ways to apply Feature Selection.  These range from simple methods such as a <em>Correlation Matrix</em> showing variable relationships, to <em>Univariate Testing</em> which helps us understand statistical relationships between variables, and then to even more powerful approaches like <em>Recursive Feature Elimination (RFE)</em> which is an approach that starts with all input variables, and then iteratively removes those with the weakest relationships with the output variable.</p>

<p>For our task we applied a variation of Reursive Feature Elimination called <em>Recursive Feature Elimination With Cross Validation (RFECV)</em> where we split the data into many “chunks” and iteratively trains &amp; validates models on each “chunk” seperately.  This means that each time we assess different models with different variables included, or eliminated, the algorithm also knows how accurate each of those models was.  From the suite of model scenarios that are created, the algorithm can determine which provided the best accuracy, and thus can infer the best set of input variables to use!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate RFECV &amp; the model type to be utilised
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">feature_selector</span> <span class="o">=</span> <span class="nc">RFECV</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>

<span class="c1"># fit RFECV onto our training &amp; test data
</span><span class="n">fit</span> <span class="o">=</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># extract &amp; print the optimal number of features
</span><span class="n">optimal_feature_count</span> <span class="o">=</span> <span class="n">feature_selector</span><span class="p">.</span><span class="n">n_features_</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal number of features: </span><span class="si">{</span><span class="n">optimal_feature_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># limit our training &amp; test sets to only include the selected variables
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">get_support</span><span class="p">()]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">get_support</span><span class="p">()]</span>

</code></pre></div></div>

<p><br />
The below code then produces a plot that visualises the cross-validated classification accuracy with each potential number of features</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">'</span><span class="s">seaborn-poster</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">"</span><span class="s">o</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Classification Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Features</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Feature Selection using RFECV </span><span class="se">\n</span><span class="s"> Optimal number of features is </span><span class="si">{</span><span class="n">optimal_feature_count</span><span class="si">}</span><span class="s"> (at score of </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">]),</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
This creates the below plot, which shows us that the highest cross-validated classification accuracy (0.904) is when we include seven of our original input variables.  The variable that has been dropped is <em>total_sales</em> but from the chart we can see that the difference is negligible.  However, we will continue on with the selected seven!</p>

<p><br />
<img src="/assets/img/project-images/log-reg-feature-selection-plot.png" alt="alt text" title="Logistic Regression Feature Selection Plot" /></p>

<p><br /></p>
<h3 id="model-training-">Model Training <a name="logreg-model-training"></a></h3>

<p>Instantiating and training our Logistic Regression model is done using the below code.  We use the <em>random_state</em> parameter to ensure reproducible results, meaning any refinements can be compared to past results.  We also specify <em>max_iter = 1000</em> to allow the solver more attempts at finding an optimal regression line, as the default value of 100 was not enough.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate our model object
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">max_iter</span> <span class="o">=</span> <span class="mi">1000</span><span class="p">)</span>

<span class="c1"># fit our model using our training &amp; test sets
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-performance-assessment-">Model Performance Assessment <a name="logreg-model-assessment"></a></h3>

<h5 id="predict-on-the-test-set">Predict On The Test Set</h5>

<p>To assess how well our model is predicting on new data - we use the trained model object (here called <em>clf</em>) and ask it to predict the <em>signup_flag</em> variable for the test set.</p>

<p>In the code below we create one object to hold the binary 1/0 predictions, and another to hold the actual prediction probabilities for the positive class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># predict on the test set
</span><span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="confusion-matrix">Confusion Matrix</h5>

<p>A Confusion Matrix provides us a visual way to understand how our predictions match up against the actual values for those test set observations.</p>

<p>The below code creates the Confusion Matrix using the <em>confusion_matrix</em> functionality from within scikit-learn and then plots it using matplotlib.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create the confusion matrix
</span><span class="n">conf_matrix</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># plot the confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-poster</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">tick_bottom</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Actual Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Class</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">corr_value</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">ndenumerate</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">corr_value</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
<img src="/assets/img/project-images/log-reg-confusion-matrix.png" alt="alt text" title="Logistic Regression Confusion Matrix" /></p>

<p><br />
The aim is to have a high proportion of observations falling into the top left cell (predicted non-signup and actual non-signup) and the bottom right cell (predicted signup and actual signup).</p>

<p>Since the proportion of signups in our data was around 30:70 we will next analyse not only Classification Accuracy, but also Precision, Recall, and F1-Score which will help us assess how well our model has performed in reality.</p>

<p><br /></p>
<h5 id="classification-performance-metrics">Classification Performance Metrics</h5>
<p><br />
<strong>Classification Accuracy</strong></p>

<p>Classification Accuracy is a metric that tells us <em>of all predicted observations, what proportion did we correctly classify</em>.  This is very intuitive, but when dealing with imbalanced classes, can be misleading.</p>

<p>An example of this could be a rare disease. A model with a 98% Classification Accuracy on might appear like a fantastic result, but if our data contained 98% of patients <em>without</em> the disease, and 2% <em>with</em> the disease - then a 98% Classification Accuracy could be obtained simply by predicting that <em>no one</em> has the disease - which wouldn’t be a great model in the real world.  Luckily, there are other metrics which can help us!</p>

<p>In this example of the rare disease, we could define Classification Accuracy as <em>of all predicted patients, what proportion did we correctly classify as either having the disease, or not having the disease</em></p>

<p><br />
<strong>Precision &amp; Recall</strong></p>

<p>Precision is a metric that tells us <em>of all observations that were predicted as positive, how many actually were positive</em></p>

<p>Keeping with the rare disease example, Precision would tell us <em>of all patients we predicted to have the disease, how many actually did</em></p>

<p>Recall is a metric that tells us <em>of all positive observations, how many did we predict as positive</em></p>

<p>Again, referring to the rare disease example, Recall would tell us <em>of all patients who actually had the disease, how many did we correctly predict</em></p>

<p>The tricky thing about Precision &amp; Recall is that it is impossible to optimise both - it’s a zero-sum game.  If you try to increase Precision, Recall decreases, and vice versa.  Sometimes however it will make more sense to try and elevate one of them, in spite of the other.  In the case of our rare-disease prediction like we’ve used in our example, perhaps it would be more important to optimise for Recall as we want to classify as many positive cases as possible.  In saying this however, we don’t want to just classify every patient as having the disease, as that isn’t a great outcome either!</p>

<p>So - there is one more metric we will discuss &amp; calculate, which is actually a <em>combination</em> of both…</p>

<p><br />
<strong>F1 Score</strong></p>

<p>F1-Score is a metric that essentially “combines” both Precision &amp; Recall.  Technically speaking, it is the harmonic mean of these two metrics.  A good, or high, F1-Score comes when there is a balance between Precision &amp; Recall, rather than a disparity between them.</p>

<p>Overall, optimising your model for F1-Score means that you’ll get a model that is working well for both positive &amp; negative classifications rather than skewed towards one or the other.  To return to the rare disease predictions, a high F1-Score would mean we’ve got a good balance between successfully predicting the disease when it’s present, and not predicting cases where it’s not present.</p>

<p>Using all of these metrics in combination gives a really good overview of the performance of a classification model, and gives us an understanding of the different scenarios &amp; considerations!</p>

<p><br />
In the code below, we utilise in-built functionality from scikit-learn to calculate these four metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># classification accuracy
</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># precision
</span><span class="nf">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># recall
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># f1-score
</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
Running this code gives us:</p>

<ul>
  <li>Classification Accuracy = <strong>0.866</strong> meaning we correctly predicted the class of 86.6% of test set observations</li>
  <li>Precision = <strong>0.784</strong> meaning that for our <em>predicted</em> delivery club signups, we were correct 78.4% of the time</li>
  <li>Recall = <strong>0.69</strong> meaning that of all <em>actual</em> delivery club signups, we predicted correctly 69% of the time</li>
  <li>F1-Score = <strong>0.734</strong></li>
</ul>

<p>Since our data is <em>somewhat</em> imbalanced, looking at these metrics rather than just Classification Accuracy on it’s own - is a good idea, and gives us a much better understanding of what our predictions mean!  We will use these same metrics when applying other models for this task, and can compare how they stack up.</p>

<p><br /></p>
<h3 id="finding-the-optimal-classification-threshold-">Finding The Optimal Classification Threshold <a name="logreg-opt-threshold"></a></h3>

<p>By default, most pre-built classification models &amp; algorithms will just use a 50% probability to discern between a positive class prediction (delivery club signup) and a negative class prediction (delivery club non-signup).</p>

<p>Just because 50% is the default threshold <em>does not mean</em> it is the best one for our task.</p>

<p>Here, we will test many potential classification thresholds, and plot the Precision, Recall &amp; F1-Score, and find an optimal solution!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># set up the list of thresholds to loop through
</span><span class="n">thresholds</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">)</span>

<span class="c1"># create empty lists to append the results to
</span><span class="n">precision_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">recall_scores</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">f1_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># loop through each threshold - fit the model - append the results
</span><span class="k">for</span> <span class="n">threshold</span> <span class="ow">in</span> <span class="n">thresholds</span><span class="p">:</span>
    
    <span class="n">pred_class</span> <span class="o">=</span> <span class="p">(</span><span class="n">y_pred_prob</span> <span class="o">&gt;=</span> <span class="n">threshold</span><span class="p">)</span> <span class="o">*</span> <span class="mi">1</span>
    
    <span class="n">precision</span> <span class="o">=</span> <span class="nf">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_class</span><span class="p">,</span> <span class="n">zero_division</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
    <span class="n">precision_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">precision</span><span class="p">)</span>
    
    <span class="n">recall</span> <span class="o">=</span> <span class="nf">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_class</span><span class="p">)</span>
    <span class="n">recall_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">recall</span><span class="p">)</span>
    
    <span class="n">f1</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">pred_class</span><span class="p">)</span>
    <span class="n">f1_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">f1</span><span class="p">)</span>
    
<span class="c1"># extract the optimal f1-score (and it's index)
</span><span class="n">max_f1</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">f1_scores</span><span class="p">)</span>
<span class="n">max_f1_idx</span> <span class="o">=</span> <span class="n">f1_scores</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">max_f1</span><span class="p">)</span>

</code></pre></div></div>
<p><br /></p>

<p>Now we have run this, we can use the below code to plot the results!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># plot the results
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-poster</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">precision_scores</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Precision</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">recall_scores</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">Recall</span><span class="sh">"</span><span class="p">,</span> <span class="n">linestyle</span> <span class="o">=</span> <span class="sh">"</span><span class="s">--</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">thresholds</span><span class="p">,</span> <span class="n">f1_scores</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sh">"</span><span class="s">F1</span><span class="sh">"</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Finding the Optimal Threshold for Classification Model </span><span class="se">\n</span><span class="s"> Max F1: </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">max_f1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s"> (Threshold = </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">thresholds</span><span class="p">[</span><span class="n">max_f1_idx</span><span class="p">],</span><span class="mi">2</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Threshold</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Assessment Score</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">legend</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="sh">"</span><span class="s">lower left</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>
<p><br />
<img src="/assets/img/project-images/log-reg-optimal-threshold-plot.png" alt="alt text" title="Logistic Regression Optimal Threshold Plot" /></p>

<p><br />
Along the x-axis of the above plot we have the different classification thresholds that were testing.  Along the y-axis we have the performance score for each of our three metrics.  As per the legend, we have Precision as a blue dotted line, Recall as an orange dotted line, and F1-Score as a thick green line.  You can see the interesting “zero-sum” relationship between Precision &amp; Recall <em>and</em> you can see that the point where Precision &amp; Recall meet is where F1-Score is maximised.</p>

<p>As you can see at the top of the plot, the optimal F1-Score for this model 0.78 and this is obtained at a classification threshold of 0.44.  This is higher than the F1-Score of 0.734 that we achieved at the default classification threshold of 0.50!</p>

<hr />
<p><br /></p>
<h1 id="decision-tree-">Decision Tree <a name="clftree-title"></a></h1>

<p>We will again utlise the scikit-learn library within Python to model our data using a Decision Tree. The code sections below are broken up into 6 key sections:</p>

<ul>
  <li>Data Import</li>
  <li>Data Preprocessing</li>
  <li>Model Training</li>
  <li>Performance Assessment</li>
  <li>Tree Visualisation</li>
  <li>Decision Tree Regularisation</li>
</ul>

<p><br /></p>
<h3 id="data-import--1">Data Import <a name="clftree-import"></a></h3>

<p>Since we saved our modelling data as a pickle file, we import it.  We ensure we remove the id column, and we also ensure our data is shuffled.</p>

<p>Just like we did for Logistic Regression - our code also investigates the class balance of our dependent variable.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span><span class="p">,</span> <span class="n">plot_tree</span>
<span class="kn">from</span> <span class="n">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>

<span class="c1"># import modelling data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/delivery_club_modelling.p</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># drop uneccessary columns
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_id</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># shuffle data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">data_for_model</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="c1"># assess class balance of dependent variable
</span><span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">(</span><span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>
<p><br /></p>
<h3 id="data-preprocessing--1">Data Preprocessing <a name="clftree-preprocessing"></a></h3>

<p>While Logistic Regression is susceptible to the effects of outliers, and highly correlated input variables - Decision Trees are not, so the required preprocessing here is lighter. We still however will put in place logic for:</p>

<ul>
  <li>Missing values in the data</li>
  <li>Encoding categorical variables to numeric form</li>
</ul>

<p><br /></p>
<h5 id="missing-values-1">Missing Values</h5>

<p>The number of missing values in the data was extremely low, so instead of applying any imputation (i.e. mean, most common value) we will just remove those rows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># remove rows where values are missing
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">data_for_model</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span> <span class="o">=</span> <span class="sh">"</span><span class="s">any</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="split-out-data-for-modelling-1">Split Out Data For Modelling</h5>

<p>In exactly the same way we did for Logistic Regression, in the next code block we do two things, we firstly split our data into an <strong>X</strong> object which contains only the predictor variables, and a <strong>y</strong> object that contains only our dependent variable.</p>

<p>Once we have done this, we split our data into training and test sets to ensure we can fairly validate the accuracy of the predictions on data that was not used in training.  In this case, we have allocated 80% of the data for training, and the remaining 20% for validation.  Again, we make sure to add in the <em>stratify</em> parameter to ensure that both our training and test sets have the same proportion of customers who did, and did not, sign up for the <em>delivery club</em> - meaning we can be more confident in our assessment of predictive performance.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># split data into X and y objects for modelling
</span><span class="n">X</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># split out training &amp; test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="categorical-predictor-variables-1">Categorical Predictor Variables</h5>

<p>In our dataset, we have one categorical variable <em>gender</em> which has values of “M” for Male, “F” for Female, and “U” for Unknown.</p>

<p>Just like the Logisitc Regression algorithm, the Decision Tree cannot deal with data in this format as it can’t assign any numerical meaning to it when looking to assess the relationship between the variable and the dependent variable.</p>

<p>As <em>gender</em> doesn’t have any explicit <em>order</em> to it, in other words, Male isn’t higher or lower than Female and vice versa - we would again apply One Hot Encoding to the categorical column.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># list of categorical variables that need encoding
</span><span class="n">categorical_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># instantiate OHE class
</span><span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop</span> <span class="o">=</span> <span class="sh">"</span><span class="s">first</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># apply OHE
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>
<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>

<span class="c1"># extract feature names for encoded columns
</span><span class="n">encoder_feature_names</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">)</span>

<span class="c1"># turn objects back to pandas dataframe
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_train_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_test_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-training--1">Model Training <a name="clftree-model-training"></a></h3>

<p>Instantiating and training our Decision Tree model is done using the below code.  We use the <em>random_state</em> parameter to ensure we get reproducible results, and this helps us understand any improvements in performance with changes to model hyperparameters.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate our model object
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">max_depth</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># fit our model using our training &amp; test sets
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-performance-assessment--1">Model Performance Assessment <a name="clftree-model-assessment"></a></h3>

<h5 id="predict-on-the-test-set-1">Predict On The Test Set</h5>

<p>Just like we did with Logistic Regression, to assess how well our model is predicting on new data - we use the trained model object (here called <em>clf</em>) and ask it to predict the <em>signup_flag</em> variable for the test set.</p>

<p>In the code below we create one object to hold the binary 1/0 predictions, and another to hold the actual prediction probabilities for the positive class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># predict on the test set
</span><span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="confusion-matrix-1">Confusion Matrix</h5>

<p>As we discussed in the above section applying Logistic Regression - a Confusion Matrix provides us a visual way to understand how our predictions match up against the actual values for those test set observations.</p>

<p>The below code creates the Confusion Matrix using the <em>confusion_matrix</em> functionality from within scikit-learn and then plots it using matplotlib.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create the confusion matrix
</span><span class="n">conf_matrix</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># plot the confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-poster</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">tick_bottom</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Actual Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Class</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">corr_value</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">ndenumerate</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">corr_value</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
<img src="/assets/img/project-images/clf-tree-confusion-matrix.png" alt="alt text" title="Decision Tree Confusion Matrix" /></p>

<p><br />
The aim is to have a high proportion of observations falling into the top left cell (predicted non-signup and actual non-signup) and the bottom right cell (predicted signup and actual signup).</p>

<p>Since the proportion of signups in our data was around 30:70 we will again analyse not only Classification Accuracy, but also Precision, Recall, and F1-Score as they will help us assess how well our model has performed from different points of view.</p>

<p><br /></p>
<h5 id="classification-performance-metrics-1">Classification Performance Metrics</h5>
<p><br />
<strong>Accuracy, Precision, Recall, F1-Score</strong></p>

<p>For details on these performance metrics, please see the above section on Logistic Regression.  Using all four of these metrics in combination gives a really good overview of the performance of a classification model, and gives us an understanding of the different scenarios &amp; considerations!</p>

<p>In the code below, we utilise in-built functionality from scikit-learn to calculate these four metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># classification accuracy
</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># precision
</span><span class="nf">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># recall
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># f1-score
</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
Running this code gives us:</p>

<ul>
  <li>Classification Accuracy = <strong>0.929</strong> meaning we correctly predicted the class of 92.9% of test set observations</li>
  <li>Precision = <strong>0.885</strong> meaning that for our <em>predicted</em> delivery club signups, we were correct 88.5% of the time</li>
  <li>Recall = <strong>0.885</strong> meaning that of all <em>actual</em> delivery club signups, we predicted correctly 88.5% of the time</li>
  <li>F1-Score = <strong>0.885</strong></li>
</ul>

<p>These are all higher than what we saw when applying Logistic Regression, even after we had optimised the classification threshold!</p>

<p><br /></p>
<h3 id="visualise-our-decision-tree-">Visualise Our Decision Tree <a name="clftree-visualise"></a></h3>

<p>To see the decisions that have been made in the tree, we can use the plot_tree functionality that we imported from scikit-learn.  To do this, we use the below code:</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># plot the nodes of the decision tree
</span><span class="n">plt</span><span class="p">.</span><span class="nf">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">tree</span> <span class="o">=</span> <span class="nf">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span>
                 <span class="n">feature_names</span> <span class="o">=</span> <span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">,</span>
                 <span class="n">filled</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">rounded</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span>
                 <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">16</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
That code gives us the below plot:</p>

<p><br />
<img src="/assets/img/project-images/clf-tree-nodes-plot.png" alt="alt text" title="Decision Tree Max Depth Plot" /></p>

<p><br />
This is a very powerful visual, and one that can be shown to stakeholders in the business to ensure they understand exactly what is driving the predictions.</p>

<p>One interesting thing to note is that the <em>very first split</em> appears to be using the variable <em>distance from store</em> so it would seem that this is a very important variable when it comes to predicting signups to the delivery club!</p>

<p><br /></p>
<h3 id="decision-tree-regularisation-">Decision Tree Regularisation <a name="clftree-model-regularisation"></a></h3>

<p>Decision Tree’s can be prone to over-fitting, in other words, without any limits on their splitting, they will end up learning the training data perfectly.  We would much prefer our model to have a more <em>generalised</em> set of rules, as this will be more robust &amp; reliable when making predictions on <em>new</em> data.</p>

<p>One effective method of avoiding this over-fitting, is to apply a <em>max depth</em> to the Decision Tree, meaning we only allow it to split the data a certain number of times before it is required to stop.</p>

<p>We initially trained our model with a placeholder depth of 5, but unfortunately, we don’t necessarily know the <em>optimal</em> number for this.  Below we will loop over a variety of values and assess which gives us the best predictive performance!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># finding the best max_depth
</span>
<span class="c1"># set up range for search, and empty list to append accuracy scores to
</span><span class="n">max_depth_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">15</span><span class="p">))</span>
<span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># loop through each possible depth, train and validate model, append test set f1-score
</span><span class="k">for</span> <span class="n">depth</span> <span class="ow">in</span> <span class="n">max_depth_list</span><span class="p">:</span>
    
    <span class="n">clf</span> <span class="o">=</span> <span class="nc">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">depth</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">accuracy_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    
<span class="c1"># store max accuracy, and optimal depth    
</span><span class="n">max_accuracy</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">accuracy_scores</span><span class="p">)</span>
<span class="n">max_accuracy_idx</span> <span class="o">=</span> <span class="n">accuracy_scores</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">max_accuracy</span><span class="p">)</span>
<span class="n">optimal_depth</span> <span class="o">=</span> <span class="n">max_depth_list</span><span class="p">[</span><span class="n">max_accuracy_idx</span><span class="p">]</span>

<span class="c1"># plot accuracy by max depth
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">max_depth_list</span><span class="p">,</span><span class="n">accuracy_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">optimal_depth</span><span class="p">,</span> <span class="n">max_accuracy</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Accuracy (F1 Score) by Max Depth </span><span class="se">\n</span><span class="s"> Optimal Tree Depth: </span><span class="si">{</span><span class="n">optimal_depth</span><span class="si">}</span><span class="s"> (F1 Score: </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">max_accuracy</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Max Depth of Decision Tree</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy (F1 Score)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>
<p><br />
That code gives us the below plot - which visualises the results!</p>

<p><br />
<img src="/assets/img/project-images/clf-tree-max-depth-plot.png" alt="alt text" title="Decision Tree Max Depth Plot" /></p>

<p><br />
In the plot we can see that the <em>maximum</em> F1-Score on the test set is found when applying a <em>max_depth</em> value of 9 which takes our F1-Score up to 0.925</p>

<hr />
<p><br /></p>
<h1 id="random-forest-">Random Forest <a name="rf-title"></a></h1>

<p>We will again utlise the scikit-learn library within Python to model our data using a Random Forest. The code sections below are broken up into 4 key sections:</p>

<ul>
  <li>Data Import</li>
  <li>Data Preprocessing</li>
  <li>Model Training</li>
  <li>Performance Assessment</li>
</ul>

<p><br /></p>
<h3 id="data-import--2">Data Import <a name="rf-import"></a></h3>

<p>Again, since we saved our modelling data as a pickle file, we import it.  We ensure we remove the id column, and we also ensure our data is shuffled.</p>

<p>As this is the exact same process we ran for both Logistic Regression &amp; the Decision Tree - our code also investigates the class balance of our dependent variable</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span>
<span class="kn">from</span> <span class="n">sklearn.inspection</span> <span class="kn">import</span> <span class="n">permutation_importance</span>

<span class="c1"># import modelling data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/delivery_club_modelling.p</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># drop uneccessary columns
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_id</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># shuffle data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">data_for_model</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

</code></pre></div></div>
<p><br /></p>
<h3 id="data-preprocessing--2">Data Preprocessing <a name="rf-preprocessing"></a></h3>

<p>While Linear Regression is susceptible to the effects of outliers, and highly correlated input variables - Random Forests, just like Decision Trees, are not, so the required preprocessing here is lighter. We still however will put in place logic for:</p>

<ul>
  <li>Missing values in the data</li>
  <li>Encoding categorical variables to numeric form</li>
</ul>

<p><br /></p>
<h5 id="missing-values-2">Missing Values</h5>

<p>The number of missing values in the data was extremely low, so instead of applying any imputation (i.e. mean, most common value) we will just remove those rows.  Again, this is exactly the same process we ran for Logistic Regression &amp; the Decision Tree.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># remove rows where values are missing
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">data_for_model</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span> <span class="o">=</span> <span class="sh">"</span><span class="s">any</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="split-out-data-for-modelling-2">Split Out Data For Modelling</h5>

<p>In exactly the same way we did for both Logistic Regression &amp; our Decision Tree, in the next code block we do two things, we firstly split our data into an X object which contains only the predictor variables, and a y object that contains only our dependent variable.</p>

<p>Once we have done this, we split our data into training and test sets to ensure we can fairly validate the accuracy of the predictions on data that was not used in training. In this case, we have allocated 80% of the data for training, and the remaining 20% for validation. Again, we make sure to add in the stratify parameter to ensure that both our training and test sets have the same proportion of customers who did, and did not, sign up for the delivery club - meaning we can be more confident in our assessment of predictive performance.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># split data into X and y objects for modelling
</span><span class="n">X</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># split out training &amp; test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="categorical-predictor-variables-2">Categorical Predictor Variables</h5>

<p>In our dataset, we have one categorical variable <em>gender</em> which has values of “M” for Male, “F” for Female, and “U” for Unknown.</p>

<p>Just like the Logistic Regression algorithm, Random Forests cannot deal with data in this format as it can’t assign any numerical meaning to it when looking to assess the relationship between the variable and the dependent variable.</p>

<p>As <em>gender</em> doesn’t have any explicit <em>order</em> to it, in other words, Male isn’t higher or lower than Female and vice versa - we would again apply One Hot Encoding to the categorical column.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># list of categorical variables that need encoding
</span><span class="n">categorical_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># instantiate OHE class
</span><span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop</span> <span class="o">=</span> <span class="sh">"</span><span class="s">first</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># apply OHE
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>
<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>

<span class="c1"># extract feature names for encoded columns
</span><span class="n">encoder_feature_names</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">)</span>

<span class="c1"># turn objects back to pandas dataframe
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_train_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_test_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-training--2">Model Training <a name="rf-model-training"></a></h3>

<p>Instantiating and training our Random Forest model is done using the below code.  We use the <em>random_state</em> parameter to ensure we get reproducible results, and this helps us understand any improvements in performance with changes to model hyperparameters.</p>

<p>We also look to build more Decision Trees in the Random Forest (500) than would be done using the default value of 100.</p>

<p>Lastly, since the default scikit-learn implementation of Random Forests does not limit the number of randomly selected variables offered up for splitting at each split point in each Decision Tree - we put this in place using the <em>max_features</em> parameter.  This can always be refined later through testing, or through an approach such as gridsearch.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate our model object
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">n_estimators</span> <span class="o">=</span> <span class="mi">500</span><span class="p">,</span> <span class="n">max_features</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>

<span class="c1"># fit our model using our training &amp; test sets
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-performance-assessment--2">Model Performance Assessment <a name="rf-model-assessment"></a></h3>

<h5 id="predict-on-the-test-set-2">Predict On The Test Set</h5>

<p>Just like we did with Logistic Regression &amp; our Decision Tree, to assess how well our model is predicting on new data - we use the trained model object (here called <em>clf</em>) and ask it to predict the <em>signup_flag</em> variable for the test set.</p>

<p>In the code below we create one object to hold the binary 1/0 predictions, and another to hold the actual prediction probabilities for the positive class.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># predict on the test set
</span><span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="confusion-matrix-2">Confusion Matrix</h5>

<p>As we discussed in the above sections - a Confusion Matrix provides us a visual way to understand how our predictions match up against the actual values for those test set observations.</p>

<p>The below code creates the Confusion Matrix using the <em>confusion_matrix</em> functionality from within scikit-learn and then plots it using matplotlib.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create the confusion matrix
</span><span class="n">conf_matrix</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># plot the confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-poster</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">tick_bottom</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Actual Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Class</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">corr_value</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">ndenumerate</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">corr_value</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
<img src="/assets/img/project-images/rf-confusion-matrix.png" alt="alt text" title="Random Forest Confusion Matrix" /></p>

<p><br />
The aim is to have a high proportion of observations falling into the top left cell (predicted non-signup and actual non-signup) and the bottom right cell (predicted signup and actual signup).</p>

<p>Since the proportion of signups in our data was around 30:70 we will again analyse not only Classification Accuracy, but also Precision, Recall, and F1-Score as they will help us assess how well our model has performed from different points of view.</p>

<p><br /></p>
<h5 id="classification-performance-metrics-2">Classification Performance Metrics</h5>
<p><br />
<strong>Accuracy, Precision, Recall, F1-Score</strong></p>

<p>For details on these performance metrics, please see the above section on Logistic Regression.  Using all four of these metrics in combination gives a really good overview of the performance of a classification model, and gives us an understanding of the different scenarios &amp; considerations!</p>

<p>In the code below, we utilise in-built functionality from scikit-learn to calculate these four metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># classification accuracy
</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># precision
</span><span class="nf">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># recall
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># f1-score
</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
Running this code gives us:</p>

<ul>
  <li>Classification Accuracy = <strong>0.935</strong> meaning we correctly predicted the class of 93.5% of test set observations</li>
  <li>Precision = <strong>0.887</strong> meaning that for our <em>predicted</em> delivery club signups, we were correct 88.7% of the time</li>
  <li>Recall = <strong>0.904</strong> meaning that of all <em>actual</em> delivery club signups, we predicted correctly 90.4% of the time</li>
  <li>F1-Score = <strong>0.895</strong></li>
</ul>

<p>These are all higher than what we saw when applying Logistic Regression, and marginally higher than what we got from our Decision Tree.  If we are after out and out accuracy then this would be the best model to choose.  If we were happier with a simpler, easier explain model, but that had almost the same performance - then we may choose the Decision Tree instead!</p>

<p><br /></p>
<h3 id="feature-importance-">Feature Importance <a name="rf-model-feature-importance"></a></h3>

<p>Random Forests are an ensemble model, made up of many, many Decision Trees, each of which is different due to the randomness of the data being provided, and the random selection of input variables available at each potential split point.</p>

<p>Because of this, we end up with a powerful and robust model, but because of the random or different nature of all these Decision trees - the model gives us a unique insight into how important each of our input variables are to the overall model.</p>

<p>As we’re using random samples of data, and input variables for each Decision Tree - there are many scenarios where certain input variables are being held back and this enables us a way to compare how accurate the models predictions are if that variable is or isn’t present.</p>

<p>So, at a high level, in a Random Forest we can measure <em>importance</em> by asking <em>How much would accuracy decrease if a specific input variable was removed or randomised?</em></p>

<p>If this decrease in performance, or accuracy, is large, then we’d deem that input variable to be quite important, and if we see only a small decrease in accuracy, then we’d conclude that the variable is of less importance.</p>

<p>At a high level, there are two common ways to tackle this.  The first, often just called <strong>Feature Importance</strong> is where we find all nodes in the Decision Trees of the forest where a particular input variable is used to split the data and assess what the gini impurity score (for a Classification problem) was before the split was made, and compare this to the gini impurity score after the split was made.  We can take the <em>average</em> of these improvements across all Decision Trees in the Random Forest to get a score that tells us <em>how much better</em> we’re making the model by using that input variable.</p>

<p>If we do this for <em>each</em> of our input variables, we can compare these scores and understand which is adding the most value to the predictive power of the model!</p>

<p>The other approach, often called <strong>Permutation Importance</strong> cleverly uses some data that has gone <em>unused</em> at when random samples are selected for each Decision Tree (this stage is called “bootstrap sampling” or “bootstrapping”)</p>

<p>These observations that were not randomly selected for each Decision Tree are known as <em>Out of Bag</em> observations and these can be used for testing the accuracy of each particular Decision Tree.</p>

<p>For each Decision Tree, all of the <em>Out of Bag</em> observations are gathered and then passed through.  Once all of these observations have been run through the Decision Tree, we obtain a classification accuracy score for these predictions.</p>

<p>In order to understand the <em>importance</em>, we <em>randomise</em> the values within one of the input variables - a process that essentially destroys any relationship that might exist between that input variable and the output variable - and run that updated data through the Decision Tree again, obtaining a second accuracy score.  The difference between the original accuracy and the new accuracy gives us a view on how important that particular variable is for predicting the output.</p>

<p><em>Permutation Importance</em> is often preferred over <em>Feature Importance</em> which can at times inflate the importance of numerical features. Both are useful, and in most cases will give fairly similar results.</p>

<p>Let’s put them both in place, and plot the results…</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># calculate feature importance
</span><span class="n">feature_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">clf</span><span class="p">.</span><span class="n">feature_importances_</span><span class="p">)</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">feature_importance_summary</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">feature_names</span><span class="p">,</span><span class="n">feature_importance</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">feature_importance_summary</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">input_variable</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">feature_importance</span><span class="sh">"</span><span class="p">]</span>
<span class="n">feature_importance_summary</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="sh">"</span><span class="s">feature_importance</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot feature importance
</span><span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">feature_importance_summary</span><span class="p">[</span><span class="sh">"</span><span class="s">input_variable</span><span class="sh">"</span><span class="p">],</span><span class="n">feature_importance_summary</span><span class="p">[</span><span class="sh">"</span><span class="s">feature_importance</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Importance of Random Forest</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Feature Importance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

<span class="c1"># calculate permutation importance
</span><span class="n">result</span> <span class="o">=</span> <span class="nf">permutation_importance</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">n_repeats</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">permutation_importance</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">result</span><span class="p">[</span><span class="sh">"</span><span class="s">importances_mean</span><span class="sh">"</span><span class="p">])</span>
<span class="n">feature_names</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">permutation_importance_summary</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">feature_names</span><span class="p">,</span><span class="n">permutation_importance</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">permutation_importance_summary</span><span class="p">.</span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">input_variable</span><span class="sh">"</span><span class="p">,</span><span class="sh">"</span><span class="s">permutation_importance</span><span class="sh">"</span><span class="p">]</span>
<span class="n">permutation_importance_summary</span><span class="p">.</span><span class="nf">sort_values</span><span class="p">(</span><span class="n">by</span> <span class="o">=</span> <span class="sh">"</span><span class="s">permutation_importance</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># plot permutation importance
</span><span class="n">plt</span><span class="p">.</span><span class="nf">barh</span><span class="p">(</span><span class="n">permutation_importance_summary</span><span class="p">[</span><span class="sh">"</span><span class="s">input_variable</span><span class="sh">"</span><span class="p">],</span><span class="n">permutation_importance_summary</span><span class="p">[</span><span class="sh">"</span><span class="s">permutation_importance</span><span class="sh">"</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Permutation Importance of Random Forest</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Permutation Importance</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>
<p><br />
That code gives us the below plots - the first being for <em>Feature Importance</em> and the second for <em>Permutation Importance</em>!</p>

<p><br />
<img src="/assets/img/project-images/rf-classification-feature-importance.png" alt="alt text" title="Random Forest Feature Importance Plot" />
<br />
<br />
<img src="/assets/img/project-images/rf-classification-permutation-importance.png" alt="alt text" title="Random Forest Permutation Importance Plot" /></p>

<p><br />
The overall story from both approaches is very similar, in that by far, the most important or impactful input variables are <em>distance_from_store</em> and <em>transaction_count</em></p>

<p>Surprisingly, <em>average_basket_size</em> was not as important as hypothesised.</p>

<p>There are slight differences in the order or “importance” for the remaining variables but overall they have provided similar findings.</p>

<hr />
<p><br /></p>
<h1 id="k-nearest-neighbours-">K Nearest Neighbours <a name="knn-title"></a></h1>

<p>We utlise the scikit-learn library within Python to model our data using KNN. The code sections below are broken up into 5 key sections:</p>

<ul>
  <li>Data Import</li>
  <li>Data Preprocessing</li>
  <li>Model Training</li>
  <li>Performance Assessment</li>
  <li>Optimal Value For K</li>
</ul>

<p><br /></p>
<h3 id="data-import--3">Data Import <a name="knn-import"></a></h3>

<p>Again, since we saved our modelling data as a pickle file, we import it. We ensure we remove the id column, and we also ensure our data is shuffled.</p>

<p>As with the other approaches, we also investigate the class balance of our dependent variable - which is important when assessing classification accuracy.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># import required packages
</span><span class="kn">import</span> <span class="n">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="n">pickle</span>
<span class="kn">import</span> <span class="n">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="n">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="kn">from</span> <span class="n">sklearn.utils</span> <span class="kn">import</span> <span class="n">shuffle</span>
<span class="kn">from</span> <span class="n">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">cross_val_score</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="n">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span><span class="p">,</span> <span class="n">accuracy_score</span><span class="p">,</span> <span class="n">precision_score</span><span class="p">,</span> <span class="n">recall_score</span><span class="p">,</span> <span class="n">f1_score</span>
<span class="kn">from</span> <span class="n">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">OneHotEncoder</span><span class="p">,</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="n">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFECV</span>

<span class="c1"># import modelling data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="n">pickle</span><span class="p">.</span><span class="nf">load</span><span class="p">(</span><span class="nf">open</span><span class="p">(</span><span class="sh">"</span><span class="s">data/delivery_club_modelling.p</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">rb</span><span class="sh">"</span><span class="p">))</span>

<span class="c1"># drop uneccessary columns
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="sh">"</span><span class="s">customer_id</span><span class="sh">"</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="c1"># shuffle data
</span><span class="n">data_for_model</span> <span class="o">=</span> <span class="nf">shuffle</span><span class="p">(</span><span class="n">data_for_model</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>

<span class="c1"># assess class balance of dependent variable
</span><span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">].</span><span class="nf">value_counts</span><span class="p">(</span><span class="n">normalize</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
From the last step in the above code, we see that <strong>69% of customers did not sign up and 31% did</strong>.  This tells us that while the data isn’t perfectly balanced at 50:50, it isn’t <em>too</em> imbalanced either.  Because of this, and as you will see, we make sure to not rely on classification accuracy alone when assessing results - also analysing Precision, Recall, and F1-Score.</p>

<p><br /></p>
<h3 id="data-preprocessing--3">Data Preprocessing <a name="knn-preprocessing"></a></h3>

<p>For KNN, as it is a distance based algorithm, we have certain data preprocessing steps that need to be addressed, including:</p>

<ul>
  <li>Missing values in the data</li>
  <li>The effect of outliers</li>
  <li>Encoding categorical variables to numeric form</li>
  <li>Feature Scaling</li>
  <li>Feature Selection</li>
</ul>

<p><br /></p>
<h5 id="missing-values-3">Missing Values</h5>

<p>The number of missing values in the data was extremely low, so instead of applying any imputation (i.e. mean, most common value) we will just remove those rows</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># remove rows where values are missing
</span><span class="n">data_for_model</span><span class="p">.</span><span class="nf">isna</span><span class="p">().</span><span class="nf">sum</span><span class="p">()</span>
<span class="n">data_for_model</span><span class="p">.</span><span class="nf">dropna</span><span class="p">(</span><span class="n">how</span> <span class="o">=</span> <span class="sh">"</span><span class="s">any</span><span class="sh">"</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="outliers-1">Outliers</h5>

<p>As KNN is a distance based algorithm, you could argue that if a data point is a long way away, then it will simply never be selected as one of the neighbours - and this is true - but outliers can still cause us problems here.  The main issue we face is when we come to scale our input variables, a very important step for a distance based algorithm.</p>

<p>We don’t want any variables to be “bunched up” due to a single outlier value, as this will make it hard to compare their values to the other input variables.  We should always investigate outliers rigorously - in this case we will simply remove them.</p>

<p>In this code section, just like we saw when applying Logistic Regression, we use <strong>.describe()</strong> from Pandas to investigate the spread of values for each of our predictors.  The results of this can be seen in the table below.</p>

<p><br /></p>

<table>
  <thead>
    <tr>
      <th><strong>metric</strong></th>
      <th><strong>distance_from_store</strong></th>
      <th><strong>credit_score</strong></th>
      <th><strong>total_sales</strong></th>
      <th><strong>total_items</strong></th>
      <th><strong>transaction_count</strong></th>
      <th><strong>product_area_count</strong></th>
      <th><strong>average_basket_value</strong></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>mean</td>
      <td>2.61</td>
      <td>0.60</td>
      <td>968.17</td>
      <td>143.88</td>
      <td>22.21</td>
      <td>4.18</td>
      <td>38.03</td>
    </tr>
    <tr>
      <td>std</td>
      <td>14.40</td>
      <td>0.10</td>
      <td>1073.65</td>
      <td>125.34</td>
      <td>11.72</td>
      <td>0.92</td>
      <td>24.24</td>
    </tr>
    <tr>
      <td>min</td>
      <td>0.00</td>
      <td>0.26</td>
      <td>2.09</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>1.00</td>
      <td>2.09</td>
    </tr>
    <tr>
      <td>25%</td>
      <td>0.73</td>
      <td>0.53</td>
      <td>383.94</td>
      <td>77.00</td>
      <td>16.00</td>
      <td>4.00</td>
      <td>21.73</td>
    </tr>
    <tr>
      <td>50%</td>
      <td>1.64</td>
      <td>0.59</td>
      <td>691.64</td>
      <td>123.00</td>
      <td>23.00</td>
      <td>4.00</td>
      <td>31.07</td>
    </tr>
    <tr>
      <td>75%</td>
      <td>2.92</td>
      <td>0.67</td>
      <td>1121.53</td>
      <td>170.50</td>
      <td>28.00</td>
      <td>5.00</td>
      <td>46.43</td>
    </tr>
    <tr>
      <td>max</td>
      <td>400.97</td>
      <td>0.88</td>
      <td>7372.06</td>
      <td>910.00</td>
      <td>75.00</td>
      <td>5.00</td>
      <td>141.05</td>
    </tr>
  </tbody>
</table>

<p><br />
Again, based on this investigation, we see some <em>max</em> column values for several variables to be much higher than the <em>median</em> value.</p>

<p>This is for columns <em>distance_from_store</em>, <em>total_sales</em>, and <em>total_items</em></p>

<p>For example, the median <em>distance_to_store</em> is 1.64 miles, but the maximum is over 400 miles!</p>

<p>Because of this, we apply some outlier removal in order to facilitate generalisation across the full dataset.</p>

<p>We do this using the “boxplot approach” where we remove any rows where the values within those columns are outside of the interquartile range multiplied by 2.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">outlier_investigation</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">describe</span><span class="p">()</span>
<span class="n">outlier_columns</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">distance_from_store</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">total_sales</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">total_items</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># boxplot approach
</span><span class="k">for</span> <span class="n">column</span> <span class="ow">in</span> <span class="n">outlier_columns</span><span class="p">:</span>
    
    <span class="n">lower_quartile</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.25</span><span class="p">)</span>
    <span class="n">upper_quartile</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">].</span><span class="nf">quantile</span><span class="p">(</span><span class="mf">0.75</span><span class="p">)</span>
    <span class="n">iqr</span> <span class="o">=</span> <span class="n">upper_quartile</span> <span class="o">-</span> <span class="n">lower_quartile</span>
    <span class="n">iqr_extended</span> <span class="o">=</span> <span class="n">iqr</span> <span class="o">*</span> <span class="mi">2</span>
    <span class="n">min_border</span> <span class="o">=</span> <span class="n">lower_quartile</span> <span class="o">-</span> <span class="n">iqr_extended</span>
    <span class="n">max_border</span> <span class="o">=</span> <span class="n">upper_quartile</span> <span class="o">+</span> <span class="n">iqr_extended</span>
    
    <span class="n">outliers</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[(</span><span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&lt;</span> <span class="n">min_border</span><span class="p">)</span> <span class="o">|</span> <span class="p">(</span><span class="n">data_for_model</span><span class="p">[</span><span class="n">column</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">max_border</span><span class="p">)].</span><span class="n">index</span>
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="si">{</span><span class="nf">len</span><span class="p">(</span><span class="n">outliers</span><span class="p">)</span><span class="si">}</span><span class="s"> outliers detected in column </span><span class="si">{</span><span class="n">column</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">outliers</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="split-out-data-for-modelling-3">Split Out Data For Modelling</h5>

<p>In exactly the same way we’ve done for the other three models, in the next code block we do two things, we firstly split our data into an X object which contains only the predictor variables, and a y object that contains only our dependent variable.</p>

<p>Once we have done this, we split our data into training and test sets to ensure we can fairly validate the accuracy of the predictions on data that was not used in training. In this case, we have allocated 80% of the data for training, and the remaining 20% for validation. Again, we make sure to add in the stratify parameter to ensure that both our training and test sets have the same proportion of customers who did, and did not, sign up for the delivery club - meaning we can be more confident in our assessment of predictive performance.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># split data into X and y objects for modelling
</span><span class="n">X</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">.</span><span class="nf">drop</span><span class="p">([</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">data_for_model</span><span class="p">[</span><span class="sh">"</span><span class="s">signup_flag</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># split out training &amp; test sets
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="nf">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">,</span> <span class="n">stratify</span> <span class="o">=</span> <span class="n">y</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="categorical-predictor-variables-3">Categorical Predictor Variables</h5>

<p>As we saw when applying the other algorithms, in our dataset, we have one categorical variable <em>gender</em> which has values of “M” for Male, “F” for Female, and “U” for Unknown.</p>

<p>The KNN algorithm can’t deal with data in this format as it can’t assign any numerical meaning to it when looking to assess the relationship between the variable and the dependent variable.</p>

<p>As <em>gender</em> doesn’t have any explicit <em>order</em> to it, in other words, Male isn’t higher or lower than Female and vice versa - one appropriate approach is to apply One Hot Encoding to the categorical column.</p>

<p>One Hot Encoding can be thought of as a way to represent categorical variables as binary vectors, in other words, a set of <em>new</em> columns for each categorical value with either a 1 or a 0 saying whether that value is true or not for that observation.  These new columns would go into our model as input variables, and the original column is discarded.</p>

<p>We also drop one of the new columns using the parameter <em>drop = “first”</em>.  We do this to avoid the <em>dummy variable trap</em> where our newly created encoded columns perfectly predict each other - and we run the risk of breaking the assumption that there is no multicollinearity, a requirement or at least an important consideration for some models, Linear Regression being one of them! Multicollinearity occurs when two or more input variables are <em>highly</em> correlated with each other, it is a scenario we attempt to avoid as in short, while it won’t neccessarily affect the predictive accuracy of our model, it can make it difficult to trust the statistics around how well the model is performing, and how much each input variable is truly having.</p>

<p>In the code, we also make sure to apply <em>fit_transform</em> to the training set, but only <em>transform</em> to the test set.  This means the One Hot Encoding logic will <em>learn and apply</em> the “rules” from the training data, but only <em>apply</em> them to the test data.  This is important in order to avoid <em>data leakage</em> where the test set <em>learns</em> information about the training data, and means we can’t fully trust model performance metrics!</p>

<p>For ease, after we have applied One Hot Encoding, we turn our training and test objects back into Pandas Dataframes, with the column names applied.</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># list of categorical variables that need encoding
</span><span class="n">categorical_vars</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">gender</span><span class="sh">"</span><span class="p">]</span>

<span class="c1"># instantiate OHE class
</span><span class="n">one_hot_encoder</span> <span class="o">=</span> <span class="nc">OneHotEncoder</span><span class="p">(</span><span class="n">sparse</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">drop</span> <span class="o">=</span> <span class="sh">"</span><span class="s">first</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># apply OHE
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>
<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[</span><span class="n">categorical_vars</span><span class="p">])</span>

<span class="c1"># extract feature names for encoded columns
</span><span class="n">encoder_feature_names</span> <span class="o">=</span> <span class="n">one_hot_encoder</span><span class="p">.</span><span class="nf">get_feature_names_out</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">)</span>

<span class="c1"># turn objects back to pandas dataframe
</span><span class="n">X_train_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_train_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_train</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_train_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_train</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

<span class="n">X_test_encoded</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">X_test_encoded</span><span class="p">,</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">encoder_feature_names</span><span class="p">)</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nf">concat</span><span class="p">([</span><span class="n">X_test</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span> <span class="n">X_test_encoded</span><span class="p">.</span><span class="nf">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="bp">True</span><span class="p">)],</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">X_test</span><span class="p">.</span><span class="nf">drop</span><span class="p">(</span><span class="n">categorical_vars</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">inplace</span> <span class="o">=</span> <span class="bp">True</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="feature-scaling">Feature Scaling</h5>

<p>As KNN is a <em>distance based</em> algorithm, in other words it is reliant on an understanding of how similar or different data points are across different dimensions in n-dimensional space, the application of <em>Feature Scaling</em> is extremely important.</p>

<p>Feature Scaling is where we force the values from different columns to exist on the same scale, in order to enchance the learning capabilities of the model. There are two common approaches for this, Standardisation, and Normalisation.</p>

<p>Standardisation rescales data to have a mean of 0, and a standard deviation of 1 - meaning most datapoints will most often fall between values of around -4 and +4.</p>

<p>Normalisation rescales datapoints so that they exist in a range between 0 and 1.</p>

<p>The below code uses the in-built <em>MinMaxScaler</em> functionality from scikit-learn to apply Normalisation to all of our input variables.  The reason we choose Normalisation over Standardisation is that our scaled data will all exist between 0 and 1, and these will then be compatible with any categorical variables that we have encoded as 1’s and 0’s.</p>

<p>In the code, we also make sure to apply <em>fit_transform</em> to the training set, but only <em>transform</em> to the test set. This means the scaling logic will learn and apply the scaling “rules” from the training data, but only apply them to the test data (or any other data we predict on in the future). This is important in order to avoid data leakage where the test set learns information about the training data, and means we can’t fully trust model performance metrics!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create our scaler object
</span><span class="n">scale_norm</span> <span class="o">=</span> <span class="nc">MinMaxScaler</span><span class="p">()</span>

<span class="c1"># normalise the training set (using fit_transform)
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">scale_norm</span><span class="p">.</span><span class="nf">fit_transform</span><span class="p">(</span><span class="n">X_train</span><span class="p">),</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

<span class="c1"># normalise the test set (using transform only)
</span><span class="n">X_test</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="nc">DataFrame</span><span class="p">(</span><span class="n">scale_norm</span><span class="p">.</span><span class="nf">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">),</span> <span class="n">columns</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">columns</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="feature-selection-1">Feature Selection</h5>

<p>As we discussed when applying Logistic Regression above - Feature Selection is the process used to select the input variables that are most important to your Machine Learning task.  For more information around this, please see that section above.</p>

<p>When applying KNN, Feature Selection is an interesting topic.  The algorithm is measuring the distance between data-points across all dimensions, where each dimension is one of our input variables.  The algorithm treats each input variable as equally important, there isn’t really a concept of “feature importance” so the spread of data within an unimportant variable could have an effect on judging other data points as either “close” or “far”.  If we had a lot of “unimportant” variables in our data, this <em>could</em> create a lot of noise for the algorithm to deal with, and we’d just see poor classification accuracy without really knowing why.</p>

<p>Having a high number of input variables also means the algorithm has to process a lot more information when processing distances between all of the data-points, so any way to reduce dimensionality is important from a computational perspective as well.</p>

<p>For our task here we are again going to apply <em>Recursive Feature Elimination With Cross Validation (RFECV)</em> which is an approach that starts with all input variables, and then iteratively removes those with the weakest relationships with the output variable.  RFECV does this using Cross Validation, so splits the data into many “chunks” and iteratively trains &amp; validates models on each “chunk” seperately.  This means that each time we assess different models with different variables included, or eliminated, the algorithm also knows how accurate each of those models was.  From the suite of model scenarios that are created, the algorithm can determine which provided the best accuracy, and thus can infer the best set of input variables to use!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate RFECV &amp; the model type to be utilised
</span><span class="kn">from</span> <span class="n">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="n">clf</span> <span class="o">=</span> <span class="nc">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
<span class="n">feature_selector</span> <span class="o">=</span> <span class="nc">RFECV</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span>

<span class="c1"># fit RFECV onto our training &amp; test data
</span><span class="n">fit</span> <span class="o">=</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>

<span class="c1"># extract &amp; print the optimal number of features
</span><span class="n">optimal_feature_count</span> <span class="o">=</span> <span class="n">feature_selector</span><span class="p">.</span><span class="n">n_features_</span>
<span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Optimal number of features: </span><span class="si">{</span><span class="n">optimal_feature_count</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># limit our training &amp; test sets to only include the selected variables
</span><span class="n">X_train</span> <span class="o">=</span> <span class="n">X_train</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">get_support</span><span class="p">()]</span>
<span class="n">X_test</span> <span class="o">=</span> <span class="n">X_test</span><span class="p">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">feature_selector</span><span class="p">.</span><span class="nf">get_support</span><span class="p">()]</span>

</code></pre></div></div>

<p><br />
The below code then produces a plot that visualises the cross-validated classification accuracy with each potential number of features</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">'</span><span class="s">seaborn-poster</span><span class="sh">'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">])</span> <span class="o">+</span> <span class="mi">1</span><span class="p">),</span> <span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">],</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">"</span><span class="s">o</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Classification Accuracy</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Number of Features</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Feature Selection using RFECV </span><span class="se">\n</span><span class="s"> Optimal number of features is </span><span class="si">{</span><span class="n">optimal_feature_count</span><span class="si">}</span><span class="s"> (at score of </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="nf">max</span><span class="p">(</span><span class="n">fit</span><span class="p">.</span><span class="n">cv_results_</span><span class="p">[</span><span class="sh">'</span><span class="s">mean_test_score</span><span class="sh">'</span><span class="p">]),</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
This creates the below plot, which shows us that the highest cross-validated classification accuracy (0.9472) is when we include six of our original input variables - although there isn’t much difference in predictive performance between using three variables through to eight variables - and this syncs with what we saw in the Random Forest section above where only three of the input variables scored highly when assessing Feature Importance &amp; Permutation Importance.</p>

<p>The variables that have been dropped are <em>total_items</em> and <em>credit score</em> - we will continue on with the remaining six!</p>

<p><br />
<img src="/assets/img/project-images/knn-feature-selection-plot.png" alt="alt text" title="KNN Feature Selection Plot" /></p>

<p><br /></p>
<h3 id="model-training--3">Model Training <a name="knn-model-training"></a></h3>

<p>Instantiating and training our KNN model is done using the below code.  At this stage we will just use the default parameters, meaning that the algorithm:</p>

<ul>
  <li>Will use a value for k of 5, or in other words it will base classifications based upon the 5 nearest neighbours</li>
  <li>Will use <em>uniform</em> weighting, or in other words an equal weighting to all 5 neighbours regardless of distance</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># instantiate our model object
</span><span class="n">clf</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">()</span>

<span class="c1"># fit our model using our training &amp; test sets
</span><span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

</code></pre></div></div>

<p><br /></p>
<h3 id="model-performance-assessment--3">Model Performance Assessment <a name="knn-model-assessment"></a></h3>

<h5 id="predict-on-the-test-set-3">Predict On The Test Set</h5>

<p>To assess how well our model is predicting on new data - we use the trained model object (here called <em>clf</em>) and ask it to predict the <em>signup_flag</em> variable for the test set.</p>

<p>In the code below we create one object to hold the binary 1/0 predictions, and another to hold the actual prediction probabilities for the positive class (which is based upon the majority class within the k nearest neighbours)</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># predict on the test set
</span><span class="n">y_pred_class</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">y_pred_prob</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict_proba</span><span class="p">(</span><span class="n">X_test</span><span class="p">)[:,</span><span class="mi">1</span><span class="p">]</span>

</code></pre></div></div>

<p><br /></p>
<h5 id="confusion-matrix-3">Confusion Matrix</h5>

<p>As we’ve seen with all models so far, our Confusion Matrix provides us a visual way to understand how our predictions match up against the actual values for those test set observations.</p>

<p>The below code creates the Confusion Matrix using the <em>confusion_matrix</em> functionality from within scikit-learn and then plots it using matplotlib.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># create the confusion matrix
</span><span class="n">conf_matrix</span> <span class="o">=</span> <span class="nf">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># plot the confusion matrix
</span><span class="n">plt</span><span class="p">.</span><span class="n">style</span><span class="p">.</span><span class="nf">use</span><span class="p">(</span><span class="sh">"</span><span class="s">seaborn-poster</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">matshow</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">,</span> <span class="n">cmap</span> <span class="o">=</span> <span class="sh">"</span><span class="s">coolwarm</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">gca</span><span class="p">().</span><span class="n">xaxis</span><span class="p">.</span><span class="nf">tick_bottom</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sh">"</span><span class="s">Confusion Matrix</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Actual Class</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Predicted Class</span><span class="sh">"</span><span class="p">)</span>
<span class="nf">for </span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">j</span><span class="p">),</span> <span class="n">corr_value</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="nf">ndenumerate</span><span class="p">(</span><span class="n">conf_matrix</span><span class="p">):</span>
    <span class="n">plt</span><span class="p">.</span><span class="nf">text</span><span class="p">(</span><span class="n">j</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="n">corr_value</span><span class="p">,</span> <span class="n">ha</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">va</span> <span class="o">=</span> <span class="sh">"</span><span class="s">center</span><span class="sh">"</span><span class="p">,</span> <span class="n">fontsize</span> <span class="o">=</span> <span class="mi">20</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>

<p><br />
<img src="/assets/img/project-images/knn-confusion-matrix.png" alt="alt text" title="KNN Confusion Matrix" /></p>

<p><br />
The aim is to have a high proportion of observations falling into the top left cell (predicted non-signup and actual non-signup) and the bottom right cell (predicted signup and actual signup).</p>

<p>The results here are interesting - all of the errors are where the model incorrectly classified <em>delivery club</em> signups as non-signups - the model made no errors when classifying non-signups non-signups.</p>

<p>Since the proportion of signups in our data was around 30:70 we will next analyse not only Classification Accuracy, but also Precision, Recall, and F1-Score which will help us assess how well our model has performed in reality.</p>

<p><br /></p>
<h5 id="classification-performance-metrics-3">Classification Performance Metrics</h5>
<p><br />
<strong>Accuracy, Precision, Recall, F1-Score</strong></p>

<p>For details on these performance metrics, please see the above section on Logistic Regression.  Using all four of these metrics in combination gives a really good overview of the performance of a classification model, and gives us an understanding of the different scenarios &amp; considerations!</p>

<p>In the code below, we utilise in-built functionality from scikit-learn to calculate these four metrics.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># classification accuracy
</span><span class="nf">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># precision
</span><span class="nf">precision_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># recall
</span><span class="nf">recall_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

<span class="c1"># f1-score
</span><span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_class</span><span class="p">)</span>

</code></pre></div></div>
<p><br />
Running this code gives us:</p>

<ul>
  <li>Classification Accuracy = <strong>0.936</strong> meaning we correctly predicted the class of 93.6% of test set observations</li>
  <li>Precision = <strong>1.00</strong> meaning that for our <em>predicted</em> delivery club signups, we were correct 100% of the time</li>
  <li>Recall = <strong>0.762</strong> meaning that of all <em>actual</em> delivery club signups, we predicted correctly 76.2% of the time</li>
  <li>F1-Score = <strong>0.865</strong></li>
</ul>

<p>These are interesting.  The KNN has obtained the highest overall Classification Accuracy &amp; Precision, but the lower Recall score has penalised the F1-Score meaning that is actually lower than what was seen for both the Decision Tree &amp; the Random Forest!</p>

<p><br /></p>
<h3 id="finding-the-optimal-value-for-k-">Finding The Optimal Value For k <a name="knn-opt-k"></a></h3>

<p>By default, the KNN algorithm within scikit-learn will use k = 5 meaning that classifications are based upon the five nearest neighbouring data-points in n-dimensional space.</p>

<p>Just because this is the default threshold <em>does not mean</em> it is the best one for our task.</p>

<p>Here, we will test many potential values for k, and plot the Precision, Recall &amp; F1-Score, and find an optimal solution!</p>

<p><br /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>
<span class="c1"># set up range for search, and empty list to append accuracy scores to
</span><span class="n">k_list</span> <span class="o">=</span> <span class="nf">list</span><span class="p">(</span><span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">25</span><span class="p">))</span>
<span class="n">accuracy_scores</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># loop through each possible value of k, train and validate model, append test set f1-score
</span><span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">k_list</span><span class="p">:</span>
    
    <span class="n">clf</span> <span class="o">=</span> <span class="nc">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span> <span class="o">=</span> <span class="n">k</span><span class="p">)</span>
    <span class="n">clf</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">clf</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
    <span class="n">accuracy</span> <span class="o">=</span> <span class="nf">f1_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span><span class="n">y_pred</span><span class="p">)</span>
    <span class="n">accuracy_scores</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">accuracy</span><span class="p">)</span>
    
<span class="c1"># store max accuracy, and optimal k value    
</span><span class="n">max_accuracy</span> <span class="o">=</span> <span class="nf">max</span><span class="p">(</span><span class="n">accuracy_scores</span><span class="p">)</span>
<span class="n">max_accuracy_idx</span> <span class="o">=</span> <span class="n">accuracy_scores</span><span class="p">.</span><span class="nf">index</span><span class="p">(</span><span class="n">max_accuracy</span><span class="p">)</span>
<span class="n">optimal_k_value</span> <span class="o">=</span> <span class="n">k_list</span><span class="p">[</span><span class="n">max_accuracy_idx</span><span class="p">]</span>

<span class="c1"># plot accuracy by max depth
</span><span class="n">plt</span><span class="p">.</span><span class="nf">plot</span><span class="p">(</span><span class="n">k_list</span><span class="p">,</span><span class="n">accuracy_scores</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">scatter</span><span class="p">(</span><span class="n">optimal_k_value</span><span class="p">,</span> <span class="n">max_accuracy</span><span class="p">,</span> <span class="n">marker</span> <span class="o">=</span> <span class="sh">"</span><span class="s">x</span><span class="sh">"</span><span class="p">,</span> <span class="n">color</span> <span class="o">=</span> <span class="sh">"</span><span class="s">red</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">title</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Accuracy (F1 Score) by k </span><span class="se">\n</span><span class="s"> Optimal Value for k: </span><span class="si">{</span><span class="n">optimal_k_value</span><span class="si">}</span><span class="s"> (Accuracy: </span><span class="si">{</span><span class="nf">round</span><span class="p">(</span><span class="n">max_accuracy</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span><span class="si">}</span><span class="s">)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">xlabel</span><span class="p">(</span><span class="sh">"</span><span class="s">k</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">ylabel</span><span class="p">(</span><span class="sh">"</span><span class="s">Accuracy (F1 Score)</span><span class="sh">"</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="nf">show</span><span class="p">()</span>

</code></pre></div></div>
<p><br />
That code gives us the below plot - which visualises the results!</p>

<p><br />
<img src="/assets/img/project-images/knn-optimal-k-value-plot.png" alt="alt text" title="KNN Optimal k Value Plot" /></p>

<p><br />
In the plot we can see that the <em>maximum</em> F1-Score on the test set is found when applying a k value of 5 - which is exactly what we started with, so nothing needs to change!</p>

<hr />
<p><br /></p>
<h1 id="modelling-summary--">Modelling Summary  <a name="modelling-summary"></a></h1>

<p>The goal for the project was to build a model that would accurately predict the customers that would sign up for the <em>delivery club</em>.  This would allow for a much more targeted approach when running the next iteration of the campaign.  A secondary goal was to understand what the drivers for this are, so the client can get closer to the customers that need or want this service, and enhance their messaging.</p>

<p>Based upon these, the chosen the model is the Random Forest as it was a) the most consistently performant on the test set across classication accuracy, precision, recall, and f1-score, and b) the feature importance and permutation importance allows the client an understanding of the key drivers behind <em>delivery club</em> signups.</p>

<p><br />
<strong>Metric 1: Classification Accuracy</strong></p>

<ul>
  <li>KNN = 0.936</li>
  <li>Random Forest = 0.935</li>
  <li>Decision Tree = 0.929</li>
  <li>Logistic Regression = 0.866</li>
</ul>

<p><br />
<strong>Metric 2: Precision</strong></p>

<ul>
  <li>KNN = 1.00</li>
  <li>Random Forest = 0.887</li>
  <li>Decision Tree = 0.885</li>
  <li>Logistic Regression = 0.784</li>
</ul>

<p><br />
<strong>Metric 3: Recall</strong></p>

<ul>
  <li>Random Forest = 0.904</li>
  <li>Decision Tree = 0.885</li>
  <li>KNN = 0.762</li>
  <li>Logistic Regression = 0.69</li>
</ul>

<p><br />
<strong>Metric 4: F1 Score</strong></p>

<ul>
  <li>Random Forest = 0.895</li>
  <li>Decision Tree = 0.885</li>
  <li>KNN = 0.865</li>
  <li>Logistic Regression = 0.734</li>
</ul>

<hr />
<p><br /></p>
<h1 id="application-">Application <a name="modelling-application"></a></h1>

<p>We now have a model object, and a the required pre-processing steps to use this model for the next <em>delivery club</em> campaign.  When this is ready to launch we can aggregate the neccessary customer information and pass it through, obtaining predicted probabilities for each customer signing up.</p>

<p>Based upon this, we can work with the client to discuss where their budget can stretch to, and contact only the customers with a high propensity to join.  This will drastically reduce marketing costs, and result in a much improved ROI.</p>

<hr />
<p><br /></p>
<h1 id="growth--next-steps-">Growth &amp; Next Steps <a name="growth-next-steps"></a></h1>

<p>While predictive accuracy was relatively high - other modelling approaches could be tested, especially those somewhat similar to Random Forest, for example XGBoost, LightGBM to see if even more accuracy could be gained.</p>

<p>We could even look to tune the hyperparameters of the Random Forest, notably regularisation parameters such as tree depth, as well as potentially training on a higher number of Decision Trees in the Random Forest.</p>

<p>From a data point of view, further variables could be collected, and further feature engineering could be undertaken to ensure that we have as much useful information available for predicting customer loyalty</p>

    </div>
</article>

    </main>

    <footer>
        <p>&copy; 2024 Sundaresh Iyer. All rights reserved.</p>
    </footer>
</body>
</html>
